# v1 - RAG Memory Provider (Feb 2026)

Open-source RAG pipeline for long-term conversational memory. No proprietary memory APIs. Benchmarked on LongMemEval (500 questions, 115k+ tokens across multi-session conversation histories).

## Architecture

```
Conversations
    |
    v
LLM Extraction (GPT-5-mini)
    |
    v
Chunking (1600 chars, 320 overlap, sentence-boundary)
    |
    v
Embedding (text-embedding-3-small, 1536 dims, batch 100)
    |
    v
Hybrid Search (BM25 * 0.3 + Vector * 0.7, overfetch 40, rerank to K=20)
    |
    v
Entity Graph (2-hop BFS, max 10 entities, max 20 relationships)
    |
    v
Answer Generation (GPT-5, reasoningEffort: medium)
    |
    v
Judge (GPT-4o)
```

### Extraction

Model: `gpt-5-mini` (reasoning model, no temperature support).

Per-session extraction via structured prompt. Outputs three XML sections:
- `<memories>`: standalone facts, each prefixed with `[YYYY-MM-DD]`. Resolves relative dates ("yesterday", "last week") to absolute dates using conversation date. One fact per line.
- `<entities>`: pipe-delimited `name|type|summary`. Types: person, organization, location, object.
- `<relationships>`: pipe-delimited `source|relation|target|date`. Relations: married_to, works_at, lives_in, etc.

Concurrency: batches of 10 sessions (`EXTRACTION_CONCURRENCY`), global cap of 300 concurrent extractions (`MAX_GLOBAL_EXTRACTIONS`). In-memory dedup cache prevents re-extracting the same session across questions.

Max retries: 5, exponential backoff (2s * 2^attempt).

### Chunking

Size: 1600 characters, overlap: 320. Sentence-boundary splitting with fallback chain: `. ` > `\n` > ` ` > hard cut at chunk size. Each chunk gets a date header: `# Memories from YYYY-MM-DD`.

Minimum overlap threshold at 50% of chunk size to prevent degenerate small overlaps.

### Embedding

Model: `text-embedding-3-small` (1536 dimensions). Batch size: 100 chunks per API call. 3 retries with linear backoff (1s, 2s, 3s).

### Hybrid Search

Two-signal fusion:
- **BM25** (weight 0.3): custom in-memory inverted index. Parameters: k1=1.2, b=0.75. Stop word removal (150+ English stop words). Tokenization: lowercase, strip non-alphanumeric, filter tokens <= 1 char.
- **Vector** (weight 0.7): cosine similarity against all chunk embeddings.

BM25 scores normalized to 0-1 range (divide by max score). Final hybrid score: `vector * 0.7 + bm25_normalized * 0.3`.

Overfetch: top 40 candidates from hybrid search, then rerank down to K=20.

### Reranker

Model: `gpt-5-mini`. Query-type-aware reranking with 7 detected types: temporal, knowledge-update, multi-hop, preference, assistant-recall, factual, general.

Each type gets specific scoring instructions (e.g., temporal: "prioritize passages with specific dates"; knowledge-update: "prioritize most recent dates").

Prompt asks for JSON array of `{index, score}` pairs, scores 0-10. Falls back to hybrid order if JSON parse fails after 3 retries.

### Entity Graph

In-memory adjacency-list graph. Entities indexed by full name and individual word parts (>2 chars) for fuzzy query matching. Word-boundary regex matching prevents false positives (e.g., "tom" inside "tomato").

Traversal: 2-hop BFS from seed entities found in query. Caps: 10 entities, 20 relationships per query. Results appended to search context as `<entities>` and `<relationships>` XML blocks.

Entity summaries concatenated across sessions (capped at 500 chars). Dedup by `source|relation|target` key.

### Answer Generation

Model: `gpt-5` with `reasoningEffort: medium`. Reasoning model (no temperature).

Prompt uses chain-of-thought format with explicit `Reasoning:` and `Answer:` sections. Context presented as `<memory session="..." date="...">` XML blocks plus optional `<knowledge_graph>` section.

### Judge

Model: `gpt-4o`. Category-specific judge prompts: default (factual), abstention, temporal, knowledge-update, preference. Binary yes/no evaluation.

### Caching

Disk persistence at `data/cache/rag/{containerTag}/`. Two files:
- `search.json`: all chunks with embeddings + serialized BM25 index (inverted index, doc lengths, stats).
- `graph.json`: all entity nodes (with session ID sets) and relationship edges.

Loaded on cache miss (first search for a container). Saved after every ingest.

## LongMemEval Results

**Overall: 82.8% (414/500)**

| Category | Questions | Correct | Accuracy |
|---|---|---|---|
| single-session-assistant | 56 | 54 | 96.4% |
| single-session-preference | 30 | 27 | 90.0% |
| single-session-user | 70 | 61 | 87.1% |
| knowledge-update | 78 | 67 | 85.9% |
| multi-session | 133 | 104 | 78.2% |
| temporal-reasoning | 133 | 101 | 75.9% |

### Retrieval Quality (K=10)

| Metric | Value |
|---|---|
| Hit@K | 96.8% |
| Recall@K | 96.8% |
| MRR | 0.906 |
| NDCG | 0.917 |

## Comparison

| Provider | Accuracy | Notes |
|---|---|---|
| Mastra Observational Memory | 94.87% | SOTA, different architecture (not RAG) |
| Supermemory | 85.9% | @DhravyaShah numbers, Feb 17 2026 |
| **Our RAG** | **82.8%** | GPT-5 + GPT-5-mini pipeline |
| OpenClaw QMD | 58.3% | |
| Filesystem (Claude code style) | 54.2% | |

## Prompt Engineering

### Chain-of-thought reasoning
Answer prompt requires explicit `Reasoning:` section before `Answer:`. Forces the model to show its work, especially for temporal and counting questions.

### Temporal date arithmetic
Prompt: "The Question Date above is YOUR reference point. Calculate ALL relative time differences from the Question Date, NOT from today." Requires explicit date math: "Event date: 2023-06-15. Question date: 2023-06-22. Difference: 7 days."

This fixed the most common temporal failure mode where the model used its training cutoff date instead of the question date.

### Preference synthesis
Prompt: "Do NOT say 'I don't know' if the context contains relevant preferences or experiences you can build on." For questions like "recommend a camera lens", the model synthesizes from known preferences (e.g., user owns a Sony camera) rather than abstaining.

### Counting/aggregation
Prompt: "List EACH item individually before giving a total. Do not estimate. Cross-check: scan ALL provided memories for mentions, not just the first few." Forces enumeration before counting.

### Knowledge-update
Prompt: "When the same topic appears at different dates with different values, the MOST RECENT memory is the current state. Earlier values are outdated."

### Multi-hop
Entity graph context injected as `<knowledge_graph>` XML. Prompt: "Use the knowledge graph to trace entity connections. Follow relationship chains."

### Reranker query type detection
Regex-based classification into 7 types. Each type gets tailored scoring instructions. Temporal queries prioritize date-bearing passages. Knowledge-update queries prioritize recent dates.

## Known Weaknesses

1. **Multi-session counting**: misses 1-2 items on "how many" questions. Retrieval returns K=20 chunks but some relevant items fall outside that window.
2. **Temporal reasoning**: relative date arithmetic fails when multiple dates are close together. Model sometimes confuses which event maps to which date.
3. **No memory versioning**: no ADD/MERGE/INVALIDATE lifecycle. Contradictory memories coexist without resolution.
4. **Single timestamp per memory**: each memory gets one date (the conversation date). No distinction between documentDate (when it was said) and eventDate (when it happened).
5. **In-memory architecture**: BM25 index and vector store live in process memory. Disk cache added for persistence, but no database backend yet.
6. **Linear scan for vector search**: cosine similarity computed against every chunk. Fine for benchmark scale (~thousands of chunks), won't work at production scale.

## Iteration History

### v1.0 - Initial Pipeline
GPT-4o for answering. Basic RAG: extraction, chunking, embedding, hybrid search, entity graph. No reranker prompt tuning. **73.2% overall.**

### v1.1 - Prompt Engineering + GPT-5
Rewrote answer prompt with chain-of-thought, temporal date arithmetic instructions, preference synthesis. Switched answering model to GPT-5 with reasoningEffort: medium. **80.8% overall.**

Major gains: temporal (+~8pp from date arithmetic fix), preferences (+~10pp from synthesis instructions), knowledge-update (+~5pp from "most recent wins" logic).

### v1.2 - Counting Fix + Multi-session Rerun
Added counting/aggregation instructions to prompt. Re-ran multi-session questions with enumeration-first approach. Multi-session: 78.2%. **Combined best: 82.8%.**

### v1.3 - Disk Persistence + Retrieval Tuning
Added disk cache for search index and entity graph (`data/cache/rag/{containerTag}/`). Increased retrieval K to 20 (from 10). Fixed async I/O race conditions in extraction pipeline. Added global extraction concurrency cap (300).

## Planned Improvements

1. **pgvector migration**: replace in-memory vector store with PostgreSQL + pgvector for production scale. ANN index (IVFFlat or HNSW) for sub-linear search.
2. **Dual timestamps**: separate documentDate (conversation date) and eventDate (when the event actually happened). Extraction prompt already resolves relative dates; store both.
3. **Memory versioning**: Mem0-style ADD/MERGE/INVALIDATE operations. When new info contradicts old, mark old as invalidated. Dedup on entity+attribute.
4. **Embedding upgrade**: voyage-3-large (1024 dims, better retrieval) or text-embedding-3-large (3072 dims). Benchmark both on retrieval metrics before switching.
5. **Contextual Retrieval prepending**: Anthropic technique. Prepend document-level context to each chunk before embedding. Reduces chunk ambiguity.
6. **Query decomposition for counting**: break "how many X" questions into sub-queries, one per potential item. Aggregate results. Should fix the 1-2 item miss.
7. **Time-aware query expansion**: for temporal questions, expand query with date ranges and temporal keywords. "What did I do last Tuesday" becomes the query plus the resolved date.
8. **Cross-encoder reranker**: replace LLM reranker with a dedicated cross-encoder model (e.g., bge-reranker-v2-m3). Faster, cheaper, potentially more consistent scores.
9. **Adaptive K**: dynamically adjust retrieval K based on query type. Counting questions get K=40, simple factual gets K=10.
